{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = {}\n",
    "path['train'] = {'order':'./training_data_2/order_data/order_data_{}', \n",
    "                    'weather': './training_data_2/weather_data/weather_data_{}',\n",
    "                    'traffic': './training_data_2/traffic_data/traffic_data_{}',\n",
    "                    'district':'./training_data_2/cluster_map/cluster_map',\n",
    "                    'poi':'./training_data_2/poi_data/poi_data'}\n",
    "path['test'] = {'order':'./test_data_2/order_data/order_data_{}_test', \n",
    "                'weather': './test_data_2/weather_data/weather_data_{}_test',\n",
    "                'traffic': './test_data_2/traffic_data/traffic_data_{}_test',\n",
    "                'district':'./test_data_2/cluster_map/cluster_map'}\n",
    "\n",
    "M = np.timedelta64(1, 'm') # base time stamp of 1 minute\n",
    "\n",
    "D_range = range(1,67) # List of all district Ids\n",
    "T_range = range(1,145) # List of all time slots\n",
    "\n",
    "test_slot1 = range(45,153,12) # Last time slot of test slot for day 23, 27, 31\n",
    "test_slot2 = range(57,153,12) # Last time slot of test slot for day 25, 29\n",
    "# List of all need to prediced time slots\n",
    "S_range = {'2016-01-23':test_slot1, '2016-01-25':test_slot2, '2016-01-27':test_slot1, \n",
    "           '2016-01-29':test_slot2, '2016-01-31':test_slot1}\n",
    "\n",
    "# Dictionary of District Info Table\n",
    "district_dict = pd.read_table(path['train']['district'], header=None, index_col=0)\n",
    "district_dict = district_dict[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print len(district_dict) == 66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def index(district, slot):\n",
    "    if type(district) is int:\n",
    "        if type(slot) is int:\n",
    "            return [x for x in itertools.product([district],[slot])]\n",
    "        else:\n",
    "            return [x for x in itertools.product([district],slot)]\n",
    "    else:\n",
    "        if type(slot) is int:\n",
    "            return [x for x in itertools.product(district,[slot])]\n",
    "        else:\n",
    "            return [x for x in itertools.product(district,slot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def District(df):\n",
    "    return df['district'].apply(lambda x: district_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Weekday(df):\n",
    "    return pd.to_datetime(df['time']).apply(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Time(df, day):\n",
    "    time = pd.to_datetime(df['time'])\n",
    "    time = (time - pd.Timestamp(day)) / M / 10 + 1\n",
    "    return time.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting testing data and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Traffic(day, option):\n",
    "    df = pd.read_table(path[option]['traffic'].format(day.date()), header=None,\n",
    "                      names=['district', 'LV1', 'LV2', 'LV3', 'LV4','time'])\n",
    "    df['district'] = District(df)\n",
    "    df['weekday'] = Weekday(df)\n",
    "    df['time'] = Time(df, str(day.date()))\n",
    "    for L in ['LV{}'.format(n) for n in range(1,5)]:\n",
    "        df[L]=df[L].apply(lambda x: x.split(':')[1]).astype(int)\n",
    "    index = pd.MultiIndex.from_arrays([df['district'].values, df['time'].values], names=('district', 'time'))\n",
    "    return pd.DataFrame({'weekday':df['weekday'].values,\n",
    "                         'day':day.day,\n",
    "                         'district':df['district'].values,\n",
    "                         'time':df['time'].values,\n",
    "                         'LV1':df['LV1'].values, \n",
    "                         'LV2':df['LV2'].values,\n",
    "                         'LV3':df['LV3'].values,\n",
    "                         'LV4':df['LV4'].values,}, index=index).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DTG: District Time Gap\n",
    "def DTG(day, option):\n",
    "    df = pd.read_table(path[option]['order'].format(day.date()), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district', 'time'])\n",
    "    df = df[df['driver'].isnull()] \n",
    "    df['district'] = District(df)\n",
    "    df['time'] = Time(df, day.date())\n",
    "    Order = df.groupby(['district', 'time'])\n",
    "    return pd.DataFrame({'gap':Order.size()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Weather(day, option):\n",
    "    df = pd.read_table(path[option]['weather'].format(day.date()), header=None,\n",
    "                      names=['time', 'weather', 'temprature', 'pm2.5'])\n",
    "    df['time'] = Time(df, day.date())\n",
    "    df = df.drop_duplicates(subset='time')\n",
    "    DF = pd.DataFrame({'time': T_range}, columns=df.columns)\n",
    "    DF = DF.set_index('time')\n",
    "    DF.update(df.set_index('time'))\n",
    "    return DF.fillna(method='bfill').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of order data for testing, indexed by date, cols = [gap]\n",
    "test_order = {} \n",
    "# Dictionary of traffic data for testing, indexed by date, cols = [weekday, district, time, LV1, LV2, LV3, LV4]\n",
    "test_traffic = {}\n",
    "# Dictionary of weather data for testing, indexed by date, cols = [temperature, weather, pm2.5]\n",
    "test_weather = {} \n",
    "for day in pd.date_range('1/23/2016', periods=5, freq='2D'):\n",
    "    test_order[str(day.date())] = DTG(day, 'test')\n",
    "    test_traffic[str(day.date())] = Traffic(day, 'test')\n",
    "    test_weather[str(day.date())] = Weather(day, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "print len(test_order.keys()) == 5\n",
    "print len(test_traffic.keys()) == 5\n",
    "print len(test_weather.keys()) == 5\n",
    "print all(test_order['2016-01-23'].columns.values == np.array(['gap']))\n",
    "print all(test_traffic['2016-01-23'].columns.values == np.array(['LV1', 'LV2', 'LV3', 'LV4', 'day', 'district', 'time', 'weekday']))\n",
    "print all(test_weather['2016-01-23'].columns.values == np.array(['weather', 'temprature', 'pm2.5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of order data for training, indexed by date, cols = [gap]\n",
    "# train_order = {}\n",
    "# Dictionary of traffic data for training, indexed by date, cols = [weekday, district, time, LV1, LV2, LV3, LV4]\n",
    "train_traffic = {}\n",
    "# Dictionary of weather data for training, indexed by date, cols = [temperature, weather, pm2.5]\n",
    "train_weather = {}\n",
    "for day in pd.date_range('1/1/2016', periods=21, freq='D'):\n",
    "#     train_order[str(day.date())] = DTG(day, 'train')\n",
    "    train_traffic[str(day.date())] = Traffic(day, 'train')\n",
    "    train_weather[str(day.date())] = Weather(day, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "print len(train_order.keys()) == 21\n",
    "print len(train_traffic.keys()) == 21\n",
    "print len(train_weather.keys()) == 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# POI2 = pd.DataFrame()\n",
    "# with open(path['train']['poi'], 'r') as f:\n",
    "#     for i, line in enumerate(f): \n",
    "#         interests = line.strip().split('\\t')\n",
    "#         row = {'district': interests[0]}\n",
    "#         for item in interests[1:]:\n",
    "#             category,num = item.split(':')\n",
    "#             if int(category.split('#')[0]) in [4,13,16,23,24]:\n",
    "#                 if category in row:\n",
    "#                     row[category] += int(num)\n",
    "#                 else:\n",
    "#                     row[category] = int(num)\n",
    "#         POI2 = pd.concat( [POI2, pd.DataFrame(row, index=[i])])\n",
    "# POI2['district'] = District(POI2)\n",
    "# POI2 = POI2.set_index('district').sort_index()\n",
    "# POI2 = POI2.fillna(0)\n",
    "# POI2.to_csv('./POI2.csv', columns=POI2.columns, header=True)\n",
    "\n",
    "POI2 = pd.read_csv('./POI2.csv', index_col='district')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# columns = ['district'] + range(1,26)\n",
    "# POI = pd.DataFrame(columns=columns)\n",
    "# with open(path['train']['poi'], 'r') as f:\n",
    "#     for i, line in enumerate(f): \n",
    "#         interests = line.strip().split('\\t')\n",
    "#         row = {'district': interests[0]}\n",
    "#         for item in interests[1:]:\n",
    "#             category,num = item.split(':')\n",
    "#             category = int(category.split('#')[0])\n",
    "#             if category in row:\n",
    "#                 row[category] += int(num)\n",
    "#             else:\n",
    "#                 row[category] = int(num)\n",
    "#         POI = pd.concat( [POI, pd.DataFrame(row, index=[i], columns=columns)])\n",
    "# POI['district'] = District(POI)\n",
    "# POI = POI.set_index('district').sort_index()\n",
    "# POI = POI.fillna(0)\n",
    "# # Standardization\n",
    "# POI = (POI - POI.mean()) / POI.std()\n",
    "# POI.to_csv('./POI.csv', columns=POI.columns, header=True)\n",
    "\n",
    "POI = pd.read_csv('./POI.csv', index_col='district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore which district is the best for replacing district 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mergin weather and gap data each day, and store it in a single csv\n",
    "# def preprocessing1(day):\n",
    "#     X = train_order[day].reindex(index(D_range,T_range)).fillna(0)\n",
    "#     X['last'] = X['gap'].shift().fillna(method='bfill')\n",
    "#     X['last2'] = X['gap'].shift(2).fillna(method='bfill') \n",
    "#     X['GAP'] = pd.Series(index=X.index)\n",
    "#     X['time'] = pd.Series(index=X.index)\n",
    "#     X['district'] = pd.Series(index=X.index)\n",
    "#     for D in D_range:\n",
    "#         X.loc[(D, T_range), 'GAP'] = X.loc[(D, T_range),'gap'].shift(-1).fillna(method='ffill')\n",
    "#         X.loc[(D, T_range), 'time'] = pd.DataFrame(T_range, columns=['time'],index=index(D, T_range))\n",
    "#         X.loc[(D, T_range), 'district'] = pd.DataFrame([D]*144, columns=['district'],index=index(D, T_range))\n",
    "#     X = X.join(train_weather[day], on='time')\n",
    "#     Y_gap = X['GAP']\n",
    "#     X.drop('GAP', axis=1, inplace=True)\n",
    "#     return X, Y_gap\n",
    "\n",
    "# for d in pd.date_range('1/2/2016', periods=20, freq='D'):\n",
    "#     X, Y_gap = preprocessing1(str(d.date()))\n",
    "    \n",
    "#     X = X.join(POI2,on='district')\n",
    "#     X.sort_index(inplace=True)\n",
    "    \n",
    "#     Y_gap.sort_index(inplace=True)\n",
    "#     X.to_csv('./X2/{}.csv'.format(str(d.date())), columns=X.columns, header=True)\n",
    "#     Y_gap.to_csv('./Y2_gap/{}.csv'.format(str(d.date())), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # from sklearn.ensemble import GradientBoostingClassifier\n",
    "# # from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# without54 = range(1,67)\n",
    "# without54.remove(54)\n",
    "# def preprocessing2(day, Mask):\n",
    "#     # filling traffic of district 54 with other district    \n",
    "#     traffic_rest = train_traffic[day].reindex(index(without54, T_range)).fillna(method='bfill')\n",
    "#     mask = Mask # The district replace district 54\n",
    "#     traffic_54 = pd.DataFrame(traffic_rest.loc[index(mask, T_range)].values, \n",
    "#                               index=index(54, T_range), columns=traffic_rest.columns)\n",
    "#     return traffic_rest, traffic_54\n",
    "\n",
    "\n",
    "# for D in without54:\n",
    "#     X_rest = []\n",
    "#     Y_gap_rest = []\n",
    "#     X_54 = []\n",
    "#     Y_gap_54 = []\n",
    "#     for d in pd.date_range('1/2/2016', periods=20, freq='D'):\n",
    "#         X_all = pd.read_csv('./X2/{}.csv'.format(str(d.date())), index_col=('district', 'time'))\n",
    "#         Y_gap_all = pd.read_csv('./Y2_gap/{}.csv'.format(str(d.date())), index_col=('district', 'time'))\n",
    "#         traffic_rest, traffic_54 = preprocessing2(str(d.date()), D)\n",
    "#         tempX_rest = pd.concat([X_all.loc[(without54, T_range),:], traffic_rest], axis=1)\n",
    "#         tempX_54 = pd.concat([X_all.loc[(54, T_range),:], traffic_54],axis=1)\n",
    "#         tempX_rest = tempX_rest.drop(['time.1','district.1'],1)\n",
    "#         tempX_54 = tempX_54.drop(['time.1','district.1'],1)\n",
    "#         X_rest.append(tempX_rest)\n",
    "#         X_54.append(tempX_54)\n",
    "#         Y_gap_rest.append(Y_gap_all.loc[(without54, T_range),:])\n",
    "#         Y_gap_54.append(Y_gap_all.loc[(54, T_range),:])\n",
    "#     X_rest = pd.concat(X_rest)\n",
    "#     X_rest.sort_index(inplace=True)\n",
    "#     X_54 = pd.concat(X_54)\n",
    "#     X_54.sort_index(inplace=True)\n",
    "#     Y_gap_rest = pd.concat(Y_gap_rest)\n",
    "#     Y_gap_rest.sort_index(inplace=True)\n",
    "#     Y_gap_54 = pd.concat(Y_gap_54)\n",
    "#     Y_gap_54.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "#     Y_gap_rest[Y_gap_rest>10]=11\n",
    "#     Y_gap_54[Y_gap_54>10]=11\n",
    "      \n",
    "#     columns = ['gap', 'last', 'last2', 'LV1', 'LV2', 'LV3', 'LV4', 'district', 'time', 'pm2.5']\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_rest[columns], Y_gap_rest, test_size=0.4)\n",
    "#     params = {'loss': 'deviance', 'learning_rate': 0.1, 'n_estimators': 20, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "#               'max_depth': 3, 'max_features': 0.8, 'subsample': 1.0, 'min_samples_split': 20, 'random_state':1,\n",
    "#               'verbose':0}\n",
    "#     grd = GradientBoostingClassifier(**params)\n",
    "#     grd.fit(X_train[columns], y_train['GAP'])\n",
    "    \n",
    "#     testX = X_54[Y_gap_54['GAP']>0][columns]\n",
    "#     testY = Y_gap_54[Y_gap_54['GAP']>0]['GAP']\n",
    "#     print \"Replaced by disctric {} : {:.8f}\".format(D,((testY - grd.predict(testX)).abs() / testY).sum() / testY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replaced by disctric 17 : 0.98912147\n",
    "# Replaced by disctric 18 : 0.98698988\n",
    "# Replaced by disctric 19 : 0.77650698\n",
    "# Replaced by disctric 20 : 0.72371558  -> 3rd\n",
    "# Replaced by disctric 21 : 0.90578814\n",
    "# Replaced by disctric 22 : 0.85776423\n",
    "# Replaced by disctric 23 : 3.11777664\n",
    "# Replaced by disctric 24 : 0.59887103  -> 1st\n",
    "# Replaced by disctric 25 : 0.74924907   \n",
    "# Replaced by disctric 26 : 0.98615067\n",
    "# Replaced by disctric 27 : 0.80867298\n",
    "# Replaced by disctric 28 : 0.61996468  -> 2nd\n",
    "# Replaced by disctric 29 : 0.85480506\n",
    "# Replaced by disctric 30 : 0.98992951\n",
    "# Replaced by disctric 31 : 0.97505635\n",
    "# Replaced by disctric 32 : 1.00273342\n",
    "# Replaced by disctric 33 : 0.99017887\n",
    "# Replaced by disctric 34 : 0.98325421\n",
    "# Replaced by disctric 35 : 0.99271328\n",
    "# Replaced by disctric 36 : 0.99436532\n",
    "# Replaced by disctric 37 : 1.04983252\n",
    "# Replaced by disctric 38 : 0.97790965\n",
    "# Replaced by disctric 39 : 0.99463147\n",
    "# Replaced by disctric 40 : 0.99509663\n",
    "# Replaced by disctric 41 : 0.98831103\n",
    "# Replaced by disctric 42 : 0.74919821\n",
    "# Replaced by disctric 43 : 0.99647533\n",
    "# Replaced by disctric 44 : 0.99103486\n",
    "# Replaced by disctric 45 : 0.98905673\n",
    "# Replaced by disctric 46 : 0.90203604\n",
    "# Replaced by disctric 47 : 0.97719033\n",
    "# Replaced by disctric 48 : 1.65585771\n",
    "# Replaced by disctric 49 : 0.99523330\n",
    "# Replaced by disctric 50 : 0.99402964\n",
    "# Replaced by disctric 51 : 3.39464397\n",
    "# Replaced by disctric 52 : 0.99622356\n",
    "# Replaced by disctric 53 : 0.99042104\n",
    "# Replaced by disctric 55 : 0.99966432\n",
    "# Replaced by disctric 56 : 0.99899295\n",
    "# Replaced by disctric 57 : 0.98358989\n",
    "# Replaced by disctric 58 : 0.99629550\n",
    "# Replaced by disctric 59 : 0.99899295\n",
    "# Replaced by disctric 60 : 0.99699324\n",
    "# Replaced by disctric 61 : 0.99699324\n",
    "# Replaced by disctric 62 : 1.00290126\n",
    "# Replaced by disctric 63 : 0.99707956\n",
    "# Replaced by disctric 64 : 0.99622356\n",
    "# Replaced by disctric 65 : 0.99874119\n",
    "# Replaced by disctric 66 : 0.99471299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filling traffic of district 54 with other district\n",
    "def replace54(day, mask):\n",
    "    without54 = range(1,67)\n",
    "    without54.remove(54)\n",
    "    traffic_rest = train_traffic[day].reindex(index(without54, T_range)).fillna(method='bfill')\n",
    "    traffic_54 = pd.DataFrame(traffic_rest.loc[index(mask, T_range)].values, \n",
    "                              index=index(54, T_range), columns=traffic_rest.columns)\n",
    "    return traffic_rest, traffic_54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Masking(Mask):\n",
    "    # Mask is the district replacing district 54\n",
    "    without54 = range(1,67)\n",
    "    without54.remove(54)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for d in pd.date_range('1/2/2016', periods=20, freq='D'):\n",
    "        X_all = pd.read_csv('./X2/{}.csv'.format(str(d.date())), index_col=('district', 'time'))\n",
    "        Y_all = pd.read_csv('./Y2_gap/{}.csv'.format(str(d.date())), index_col=('district', 'time'))\n",
    "        traffic_rest, traffic_54 = replace54(str(d.date()), Mask)\n",
    "\n",
    "        tempX_rest = pd.concat([X_all.loc[(without54, T_range),:], traffic_rest], axis=1)\n",
    "        tempX_rest = tempX_rest.drop(['time.1','district.1'],1)\n",
    "        tempX_54 = pd.concat([X_all.loc[(54, T_range),:], traffic_54],axis=1)    \n",
    "        tempX_54 = tempX_54.drop(['time.1','district.1'],1)    \n",
    "        tempX_all = pd.concat([tempX_rest, tempX_54]).sort_index()\n",
    "        X.append(tempX_all)\n",
    "\n",
    "        tempY_rest = Y_all.loc[(without54, T_range),:]\n",
    "        tempY_54 = Y_all.loc[(54, T_range),:]\n",
    "        tempY_all = pd.concat([tempY_rest, tempY_54]).sort_index()\n",
    "        Y.append(tempY_all)\n",
    "    X = pd.concat(X)\n",
    "    Y = pd.concat(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore which district is better on POI2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced by disctric 1 : 0.43134926\n",
      "Replaced by disctric 2 : 0.43239209\n",
      "Replaced by disctric 3 : 0.43662162\n",
      "Replaced by disctric 4 : 0.43378158\n",
      "Replaced by disctric 5 : 0.43164034\n",
      "Replaced by disctric 6 : 0.43288644\n",
      "Replaced by disctric 7 : 0.42900990\n",
      "Replaced by disctric 8 : 0.43107097\n",
      "Replaced by disctric 9 : 0.42519920\n",
      "Replaced by disctric 10 : 0.43102830\n",
      "Replaced by disctric 11 : 0.42780553\n",
      "Replaced by disctric 12 : 0.43088756\n",
      "Replaced by disctric 13 : 0.43555360\n",
      "Replaced by disctric 14 : 0.43099010\n",
      "Replaced by disctric 15 : 0.43549541\n",
      "Replaced by disctric 16 : 0.43234372\n",
      "Replaced by disctric 17 : 0.43835595\n",
      "Replaced by disctric 18 : 0.43056354\n",
      "Replaced by disctric 19 : 0.43265371\n",
      "Replaced by disctric 20 : 0.43311840\n",
      "Replaced by disctric 21 : 0.43048995\n",
      "Replaced by disctric 22 : 0.43118376\n",
      "Replaced by disctric 23 : 0.43293011\n",
      "Replaced by disctric 24 : 0.43149041\n",
      "Replaced by disctric 25 : 0.43299912\n",
      "Replaced by disctric 26 : 0.43039004\n",
      "Replaced by disctric 27 : 0.43214967\n",
      "Replaced by disctric 28 : 0.42867821\n",
      "Replaced by disctric 29 : 0.42982001\n",
      "Replaced by disctric 30 : 0.43205035\n",
      "Replaced by disctric 31 : 0.43440043\n",
      "Replaced by disctric 32 : 0.43541702\n",
      "Replaced by disctric 33 : 0.43102059\n",
      "Replaced by disctric 34 : 0.43742158\n",
      "Replaced by disctric 35 : 0.43564980\n",
      "Replaced by disctric 36 : 0.42799378\n",
      "Replaced by disctric 37 : 0.42624577\n",
      "Replaced by disctric 38 : 0.43094743\n",
      "Replaced by disctric 39 : 0.43339908\n",
      "Replaced by disctric 40 : 0.43751719\n",
      "Replaced by disctric 41 : 0.43120191\n",
      "Replaced by disctric 42 : 0.43184901\n",
      "Replaced by disctric 43 : 0.43323008\n",
      "Replaced by disctric 44 : 0.43530426\n",
      "Replaced by disctric 45 : 0.43030129\n",
      "Replaced by disctric 46 : 0.43296481\n",
      "Replaced by disctric 47 : 0.43371596\n",
      "Replaced by disctric 48 : 0.43144904\n",
      "Replaced by disctric 49 : 0.43245215\n",
      "Replaced by disctric 50 : 0.43588607\n",
      "Replaced by disctric 51 : 0.43425369\n",
      "Replaced by disctric 52 : 0.43262960\n",
      "Replaced by disctric 53 : 0.43371754\n",
      "Replaced by disctric 55 : 0.43128574\n",
      "Replaced by disctric 56 : 0.43303190\n",
      "Replaced by disctric 57 : 0.43176025\n",
      "Replaced by disctric 58 : 0.43252662\n",
      "Replaced by disctric 59 : 0.42745728\n",
      "Replaced by disctric 60 : 0.43190217\n",
      "Replaced by disctric 61 : 0.43445249\n",
      "Replaced by disctric 62 : 0.43796914\n",
      "Replaced by disctric 63 : 0.43437871\n",
      "Replaced by disctric 64 : 0.43070752\n",
      "Replaced by disctric 65 : 0.43270655\n",
      "Replaced by disctric 66 : 0.43217432\n"
     ]
    }
   ],
   "source": [
    "# Best District : 9\n",
    "# without54 = range(1,67)\n",
    "# without54.remove(54)\n",
    "# for D in without54:\n",
    "#     X, Y = Masking(D)\n",
    "#     newY = Y.copy()\n",
    "#     newX = X.copy()\n",
    "#     newX = newX[newY['GAP']>0]\n",
    "#     newX = newX.drop('last2',axis=1)\n",
    "#     newY = newY[newY['GAP']>0]\n",
    "#     newY[newY['GAP']>39]=40\n",
    "#     class_weight = dict(zip(range(1,38), range(38,1,-1)))\n",
    "#     columns = ['gap', 'last', 'pm2.5', '16#4', '16#10', '24#2',\n",
    "#                'LV1', 'LV2', 'LV3', 'LV4', 'day', 'time']\n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     params = {'criterion': 'entropy', 'n_estimators': 40, 'min_samples_leaf':10, 'min_samples_split':20,\n",
    "#               'max_depth': 9, 'max_features': 0.85, 'random_state':1,\n",
    "#               'verbose':0, 'class_weight': class_weight}\n",
    "#     rfc = RandomForestClassifier(**params)\n",
    "#     rfc.fit(newX[columns], newY['GAP'])\n",
    "#     print \"Replaced by disctric {} : {:.8f}\".format(D, score_on_test_data(rfc, columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identified the most common gap growth each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DF = pd.DataFrame(columns=range(1,12))\n",
    "# for d in pd.date_range('1/2/2016', periods=20, freq='1D'):\n",
    "#     df = train_order[str(d.date())]\n",
    "#     df = df.reindex(index(D_range, T_range)).fillna(0)\n",
    "#     for D in D_range:\n",
    "#         df.loc[(D, T_range),'diff'] = df.loc[(D, T_range),'gap'].diff().shift(-1).fillna(0)\n",
    "#     row = pd.DataFrame(df['diff'].value_counts().sort_values(ascending=False).iloc[:11].index.values.reshape((1,11)),\n",
    "#                        columns=range(1,12), index=[d.day])\n",
    "#     DF = DF.append(row)\n",
    "# print DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identified the most common gap each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DF = pd.DataFrame(columns=range(1,12))\n",
    "# for d in pd.date_range('1/2/2016', periods=20, freq='1D'):\n",
    "#     df = train_order[str(d.date())]\n",
    "#     row = pd.DataFrame(df['gap'].value_counts().sort_values(ascending=False)[:11].index.values.reshape((1,11)),\n",
    "#                        columns=range(1,12), index=[d.day])\n",
    "#     DF = DF.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for d in pd.date_range('1/23/2016', periods=5, freq='2D'):\n",
    "#     df = test_order[str(d.date())]\n",
    "#     row = pd.DataFrame(df['gap'].value_counts().sort_values(ascending=False)[:11].index.values.reshape((1,11)),\n",
    "#                        columns=range(1,12), index=[d.day])\n",
    "#     DF = DF.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# columns is the top 11 most common gap\n",
    "# index is each day\n",
    "# print DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5 by Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_GBD(day, pred):\n",
    "    ans = test_order[day].reindex(index(D_range, S_range[day])).fillna(0)\n",
    "    ans = ans['gap'].values\n",
    "    pred = pred[ans>0]\n",
    "    ans = ans[ans>0]\n",
    "    gap = (ans - pred) / ans\n",
    "    return np.fabs(gap).sum()/ans.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_on_test_data(clf, columns):\n",
    "    SLOT1 = range(44,152,12)\n",
    "    SLOT2 = range(56,152,12)\n",
    "    RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "    scores = []\n",
    "    for day in RANGE.keys():\n",
    "        x = select_last_points(day, RANGE[day])\n",
    "        if len(columns) == 0:\n",
    "            score = score_GBD(day, clf.predict(x))\n",
    "        else:\n",
    "            score = score_GBD(day, clf.predict(x[columns]))\n",
    "        scores.append(score * x.shape[0])\n",
    "#         print '\\t score: {}'.format(score)\n",
    "    return np.array(scores).sum()/2838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_last_points(day, slot):\n",
    "    slot = np.array(slot)\n",
    "    x = test_order[day].reindex(index(D_range, slot)).fillna(0)\n",
    "    x['last'] = test_order[day].reindex(index(D_range, slot-1)).fillna(0)\n",
    "    x = pd.concat([x, test_traffic[day].reindex(index(D_range, slot))],axis=1)\n",
    "    # For missing traffic data on district 54, replaced by Mask\n",
    "    for t in slot:\n",
    "        x.loc[(54,t)]['LV1':'weekday'] = x.loc[(Mask,t)]['LV1':'weekday']\n",
    "    x = x.join(test_weather[day], on='time')\n",
    "    x = x.join(POI2,on='district')\n",
    "#     print \"Select data from {} on {}\".format(day, slot)\n",
    "#     print \"\\t shape: {}\".format(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Mask = 9\n",
    "X, Y = Masking(Mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX.drop('last2',1), newY['GAP'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False,\n",
       "           class_weight={1: 44, 2: 43, 3: 42, 4: 41, 5: 40, 6: 39, 7: 38, 8: 37, 9: 36, 10: 35, 11: 34, 12: 33, 13: 32, 14: 31, 15: 30, 16: 29, 17: 28, 18: 27, 19: 26, 20: 25, 21: 24, 22: 23, 23: 22, 24: 21, 25: 20, 26: 19, 27: 18, 28: 17, 29: 16, 30: 15, 31: 14, 32: 13, 33: 12, 34: 11, 35: 10, 36: 9, 37: 8, 38: 7, 39: 6, 40: 5, 41: 4, 42: 3, 43: 2},\n",
       "           criterion='entropy', max_depth=11, max_features=0.9,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = dict(zip(range(1,44), range(44,1,-1)))\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "params = {'criterion': 'entropy', 'n_estimators': 30, 'min_samples_leaf':1, 'min_samples_split':2,\n",
    "          'max_depth': 11, 'max_features': 0.9, 'random_state':1,\n",
    "          'verbose':0, 'class_weight': class_weight}\n",
    "etc = ExtraTreesClassifier(**params)\n",
    "etc.fit(newX[columns], newY['GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select data from 2016-01-31 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.47556958099\n",
      "Select data from 2016-01-23 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.458208490768\n",
      "Select data from 2016-01-29 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.490209217019\n",
      "Select data from 2016-01-27 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.423029987208\n",
      "Select data from 2016-01-25 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.464640080984\n",
      "0.461629463133\n"
     ]
    }
   ],
   "source": [
    "score_on_test_data(etc, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newY = Y.copy()\n",
    "newX = X.copy()\n",
    "newX = newX[newY['GAP']>0]\n",
    "newX = newX.drop('last2',axis=1)\n",
    "newY = newY[newY['GAP']>0]\n",
    "newY[newY['GAP']>39]=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={1: 38, 2: 37, 3: 36, 4: 35, 5: 34, 6: 33, 7: 32, 8: 31, 9: 30, 10: 29, 11: 28, 12: 27, 13: 26, 14: 25, 15: 24, 16: 23, 17: 22, 18: 21, 19: 20, 20: 19, 21: 18, 22: 17, 23: 16, 24: 15, 25: 14, 26: 13, 27: 12, 28: 11, 29: 10, 30: 9, 31: 8, 32: 7, 33: 6, 34: 5, 35: 4, 36: 3, 37: 2},\n",
       "            criterion='entropy', max_depth=9, max_features=0.85,\n",
       "            max_leaf_nodes=None, min_samples_leaf=10, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = dict(zip(range(1,38), range(38,1,-1)))\n",
    "# columns = ['gap', 'last', 'pm2.5', '16#4', '16#10', '24#2',\n",
    "#            'LV1', 'LV2', 'LV3', 'LV4', 'day', 'time']\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "params = {'criterion': 'entropy', 'n_estimators': 40, 'min_samples_leaf':10, 'min_samples_split':20,\n",
    "          'max_depth': 9, 'max_features': 0.85, 'random_state':1,\n",
    "          'verbose':0, 'class_weight': class_weight}\n",
    "rfc = RandomForestClassifier(**params)\n",
    "rfc.fit(newX, newY['GAP'])\n",
    "# rfc.fit(newX[columns], etc.predict(newX[columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gap' 'last' 'pm2.5' '16#10' '16#4' '24#2' 'LV1' 'LV2' 'LV3' 'LV4' 'day'\n",
      " 'time']\n"
     ]
    }
   ],
   "source": [
    "# print rfc.feature_importances_\n",
    "columns = newX.columns[[i for i, important in enumerate(rfc.feature_importances_ > 0.01) if important]].values\n",
    "print columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newY = Y.copy()\n",
    "newX = X.copy()\n",
    "newX = newX[newY['GAP']>0]\n",
    "newX = newX.drop('last2',axis=1)\n",
    "newY = newY[newY['GAP']>0]\n",
    "newY[newY['GAP']>39]=40\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(newX[columns], newY['GAP'], test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.423280995219\n"
     ]
    }
   ],
   "source": [
    "columns = ['gap', 'last', 'pm2.5', '16#4', '16#10', '24#2',\n",
    "               'LV1', 'LV2', 'LV3', 'LV4', 'day', 'time']\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class_weight = dict(zip(range(1,38), range(38,1,-1)))\n",
    "params = {'criterion': 'entropy', 'n_estimators': 40, 'min_samples_leaf':10, 'min_samples_split':20,\n",
    "          'max_depth': 9, 'max_features': 0.85, 'random_state':1,\n",
    "          'verbose':0, 'class_weight': class_weight}\n",
    "rfc1 = RandomForestClassifier(**params)\n",
    "rfc1.fit(newX[columns], newY['GAP'])\n",
    "print score_on_test_data(rfc1, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced by disctric 9 : 0.42519920\n",
      "Replaced by disctric 11 : 0.42780553\n",
      "Replaced by disctric 19 : 0.43265371\n",
      "Replaced by disctric 20 : 0.43311840\n",
      "Replaced by disctric 24 : 0.43149041\n",
      "Replaced by disctric 28 : 0.42867821\n",
      "Replaced by disctric 29 : 0.42982001\n",
      "Replaced by disctric 36 : 0.42799378\n",
      "Replaced by disctric 37 : 0.42624577\n",
      "Replaced by disctric 59 : 0.42745728\n"
     ]
    }
   ],
   "source": [
    "# Best District : 9\n",
    "# without54 = range(1,67)\n",
    "# without54.remove(54)\n",
    "# for D in [9,11,19,20,24,28,29,36,37,59]:\n",
    "#     X, Y = Masking(D)\n",
    "#     newY = Y.copy()\n",
    "#     newX = X.copy()\n",
    "#     newX = newX[newY['GAP']>0]\n",
    "#     newX = newX.drop('last2',axis=1)\n",
    "#     newY = newY[newY['GAP']>0]\n",
    "#     newY[newY['GAP']>39]=40\n",
    "#     class_weight = dict(zip(range(1,38), range(38,1,-1)))\n",
    "#     columns = ['gap', 'last', 'pm2.5', '16#4', '16#10', '24#2',\n",
    "#                'LV1', 'LV2', 'LV3', 'LV4', 'day', 'time']\n",
    "#     from sklearn.ensemble import RandomForestClassifier\n",
    "#     params = {'criterion': 'entropy', 'n_estimators': 40, 'min_samples_leaf':10, 'min_samples_split':20,\n",
    "#               'max_depth': 9, 'max_features': 0.85, 'random_state':1,\n",
    "#               'verbose':0, 'class_weight': class_weight}\n",
    "#     rfc = RandomForestClassifier(**params)\n",
    "#     rfc.fit(newX[columns], newY['GAP'])\n",
    "#     print \"Replaced by disctric {} : {:.8f}\".format(D, score_on_test_data(rfc, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.420953714675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "params = {'loss': 'quantile', 'alpha': 0.85, 'n_estimators':100, 'max_features':0.85, 'random_state':1,\n",
    "          'min_samples_split':10,'max_depth': 10, 'learning_rate': 0.08, 'subsample': 0.9, 'verbose':0}\n",
    "Regr = GradientBoostingRegressor(**params)\n",
    "Regr.fit(newX[columns], rfc1.predict(newX[columns]))\n",
    "print score_on_test_data(Regr, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write5(x, day, slot, rfc, mode):\n",
    "    with open('ans5_v3.csv', mode) as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for D in D_range:\n",
    "            for S in slot:\n",
    "                gap = rfc.predict(x.loc[(D,S)].reshape(1, -1))[0]\n",
    "                writer.writerow([str(D),'{}-{}'.format(day,S+1), '{:.15f}'.format(gap)])\n",
    "for day in S_range.keys():\n",
    "    x = select_last_points(day, S_range[day])\n",
    "    write5(x[columns], day, S_range[day], Regr, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36590289181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "params = {'base_estimator':Regr,'n_estimators':5, 'learning_rate':1.0, 'random_state':1, 'loss':'square'}\n",
    "adbr = AdaBoostRegressor(**params)\n",
    "adbr.fit(newX[columns], newY['GAP'])\n",
    "print score_on_test_data(adbr, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.0196           0.2006            1.73m\n",
      "         2           0.8967           0.1202            1.83m\n",
      "         3           0.8181           0.0925            1.79m\n",
      "         4           0.7182           0.1001            1.80m\n",
      "         5           0.6360           0.0865            1.82m\n",
      "         6           0.5861           0.0613            1.81m\n",
      "         7           0.5151           0.0594            1.97m\n",
      "         8           0.4645           0.0509            2.22m\n",
      "         9           0.4053           0.0511            2.35m\n",
      "        10           0.3739           0.0378            2.48m\n",
      "        20           0.1314           0.0138            2.50m\n",
      "        30           0.0353           0.0054            2.53m\n",
      "        40          -0.0031           0.0015            2.03m\n",
      "        50          -0.0168           0.0006            1.60m\n",
      "        60          -0.0206           0.0001            1.24m\n",
      "        70          -0.0213          -0.0002           54.13s\n",
      "        80          -0.0207          -0.0000           35.91s\n",
      "        90          -0.0200          -0.0001           17.64s\n",
      "       100          -0.0194          -0.0001            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.0049           0.1696            1.94m\n",
      "         2           0.8963           0.1069            1.96m\n",
      "         3           0.7880           0.1020            2.05m\n",
      "         4           0.7181           0.0739            2.07m\n",
      "         5           0.6300           0.0784            2.33m\n",
      "         6           0.5651           0.0639            2.35m\n",
      "         7           0.4991           0.0586            2.35m\n",
      "         8           0.4470           0.0526            2.33m\n",
      "         9           0.3971           0.0467            2.37m\n",
      "        10           0.3631           0.0368            2.42m\n",
      "        20           0.1168           0.0132            2.15m\n",
      "        30           0.0259           0.0039            1.81m\n",
      "        40          -0.0105           0.0016            1.52m\n",
      "        50          -0.0216           0.0004            1.25m\n",
      "        60          -0.0239          -0.0002            1.00m\n",
      "        70          -0.0233          -0.0001           44.91s\n",
      "        80          -0.0223          -0.0002           30.39s\n",
      "        90          -0.0211          -0.0002           15.23s\n",
      "       100          -0.0201          -0.0001            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.0197           0.1885            1.84m\n",
      "         2           0.9192           0.1036            1.99m\n",
      "         3           0.8121           0.0926            2.16m\n",
      "         4           0.7271           0.0904            2.28m\n",
      "         5           0.6532           0.0689            2.29m\n",
      "         6           0.5854           0.0598            2.25m\n",
      "         7           0.5132           0.0674            2.26m\n",
      "         8           0.4680           0.0486            2.28m\n",
      "         9           0.4203           0.0447            2.25m\n",
      "        10           0.3683           0.0486            2.24m\n",
      "        20           0.1333           0.0140            2.13m\n",
      "        30           0.0333           0.0073            1.80m\n",
      "        40          -0.0044           0.0026            1.56m\n",
      "        50          -0.0173           0.0007            1.33m\n",
      "        60          -0.0208           0.0002            1.04m\n",
      "        70          -0.0213          -0.0003           46.08s\n",
      "        80          -0.0210          -0.0001           31.72s\n",
      "        90          -0.0202          -0.0001           15.59s\n",
      "       100          -0.0196          -0.0001            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.0078           0.2214            2.32m\n",
      "         2           0.9268           0.0908            2.41m\n",
      "         3           0.8097           0.1015            2.46m\n",
      "         4           0.7120           0.0965            2.45m\n",
      "         5           0.6414           0.0830            2.56m\n",
      "         6           0.5769           0.0731            2.65m\n",
      "         7           0.5161           0.0587            2.69m\n",
      "         8           0.4657           0.0488            2.68m\n",
      "         9           0.4097           0.0490            2.65m\n",
      "        10           0.3659           0.0432            2.67m\n",
      "        20           0.1280           0.0151            2.30m\n",
      "        30           0.0317           0.0067            2.13m\n",
      "        40          -0.0054           0.0020            1.75m\n",
      "        50          -0.0182           0.0005            1.42m\n",
      "        60          -0.0218           0.0001            1.13m\n",
      "        70          -0.0221          -0.0001           50.97s\n",
      "        80          -0.0219          -0.0002           33.90s\n",
      "        90          -0.0203          -0.0002           17.13s\n",
      "       100          -0.0196          -0.0002            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.9936           0.1952            5.71m\n",
      "         2           0.9137           0.0911            5.58m\n",
      "         3           0.8056           0.0937            4.69m\n",
      "         4           0.7150           0.0840            4.24m\n",
      "         5           0.6263           0.0857            4.02m\n",
      "         6           0.5721           0.0653            3.82m\n",
      "         7           0.5061           0.0594            3.75m\n",
      "         8           0.4534           0.0506            3.79m\n",
      "         9           0.3973           0.0516            3.68m\n",
      "        10           0.3701           0.0356            3.58m\n",
      "        20           0.1213           0.0149            3.33m\n",
      "        30           0.0300           0.0065            2.59m\n",
      "        40          -0.0055           0.0023            2.02m\n",
      "        50          -0.0173           0.0006            1.57m\n",
      "        60          -0.0210           0.0000            1.21m\n",
      "        70          -0.0217          -0.0001           52.62s\n",
      "        80          -0.0213          -0.0001           35.00s\n",
      "        90          -0.0206          -0.0001           17.65s\n",
      "       100          -0.0203          -0.0001            0.00s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 13.8min finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.42649308167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "params = {'base_estimator':Regr, 'verbose':1,'n_estimators':5}\n",
    "\n",
    "bgr = BaggingRegressor(**params)\n",
    "bgr.fit(newX[columns], Regr.predict(newX[columns]))\n",
    "print score_on_test_data(bgr, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=3, max_features=0.8, max_leaf_nodes=None,\n",
       "              min_samples_leaf=10, min_samples_split=20,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=1, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "params = {'loss': 'deviance', 'learning_rate': 0.1, 'n_estimators': 50, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "          'max_depth': 3, 'max_features': 0.8, 'subsample': 1.0, 'min_samples_split': 20, 'random_state':1,\n",
    "          'verbose':0}\n",
    "grd = GradientBoostingClassifier(**params)\n",
    "grd.fit(newX[columns], newY['GAP'])\n",
    "print score_on_test_data(grd, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      172624.1761           16.64m\n",
      "         2      143984.9064           15.83m\n",
      "         3      124735.3358           14.93m\n",
      "         4      109674.3424           14.32m\n",
      "         5       97780.2853           13.76m\n",
      "         6       88028.6071           13.32m\n",
      "         7       79930.7874           12.98m\n",
      "         8       73394.8895           12.72m\n",
      "         9       67911.2652           12.42m\n",
      "        10       63166.6599           12.20m\n",
      "        20       38950.5329            9.00m\n",
      "        30       30167.2405            5.95m\n",
      "        40       25634.6061            2.94m\n",
      "        50       22713.8607            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      173763.3828           14.78m\n",
      "         2      145428.7745           14.36m\n",
      "         3      125591.3903           13.86m\n",
      "         4      110824.4021           13.60m\n",
      "         5       98875.8994           13.29m\n",
      "         6       89403.9278           12.99m\n",
      "         7       81524.0367           12.74m\n",
      "         8       75007.7739           12.64m\n",
      "         9       69286.0766           12.35m\n",
      "        10       64612.5094           12.07m\n",
      "        20       40385.1230            9.02m\n",
      "        30       31553.2203            5.99m\n",
      "        40       27216.2519            2.96m\n",
      "        50       27129.4854            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      170729.8553           14.00m\n",
      "         2      143300.6151           13.77m\n",
      "         3      124078.0875           13.43m\n",
      "         4      108904.5696           13.25m\n",
      "         5       96948.8987           13.02m\n",
      "         6       87518.0566           12.70m\n",
      "         7       79625.4242           12.42m\n",
      "         8       73063.4792           12.16m\n",
      "         9       67372.3198           11.87m\n",
      "        10       62567.2018           11.58m\n",
      "        20       38225.0918            8.77m\n",
      "        30       29558.6664            5.81m\n",
      "        40       25716.8432            2.91m\n",
      "        50       23955.4922            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      173724.0734           14.03m\n",
      "         2      145361.7250           13.80m\n",
      "         3      125373.1050           13.52m\n",
      "         4      110316.2540           13.26m\n",
      "         5       97975.2287           12.98m\n",
      "         6       88169.7210           12.65m\n",
      "         7       80147.0268           12.63m\n",
      "         8       73652.3165           12.38m\n",
      "         9       68063.5403           12.11m\n",
      "        10       63213.4783           11.82m\n",
      "        20       38648.7288            8.86m\n",
      "        30       29793.9327            5.92m\n",
      "        40       25188.7718            3.02m\n",
      "        50       22686.0381            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      174809.2763           16.58m\n",
      "         2      146621.2834           15.48m\n",
      "         3      126695.3826           14.83m\n",
      "         4      111324.5229           14.33m\n",
      "         5       98989.6776           13.93m\n",
      "         6       89088.5690           13.60m\n",
      "         7       81040.5279           13.22m\n",
      "         8       74343.1164           12.84m\n",
      "         9       68713.4133           12.51m\n",
      "        10       63924.5349           12.14m\n",
      "        20       39382.4675            9.01m\n",
      "        30       30483.4171            5.89m\n",
      "        40       19988.1372            2.95m\n",
      "        50       13755.1015            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 73.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=3, max_features=0.8, max_leaf_nodes=None,\n",
       "              min_samples_leaf=10, min_samples_split=20,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=1, subsample=1.0, verbose=1,\n",
       "              warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=5, n_jobs=1, oob_score=False,\n",
       "         random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "params = {'base_estimator':grd, 'verbose':1,'n_estimators':5}\n",
    "\n",
    "bgc = BaggingClassifier(**params)\n",
    "bgc.fit(newX[columns], grd.predict(newX[columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select data from 2016-01-31 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.439117662744\n",
      "Select data from 2016-01-23 on [ 44  56  68  80  92 104 116 128 140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.402798003284\n",
      "Select data from 2016-01-29 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.462474864258\n",
      "Select data from 2016-01-27 on [ 44  56  68  80  92 104 116 128 140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.401695452452\n",
      "Select data from 2016-01-25 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.426770371535\n",
      "0.425731673318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "score_on_test_data(bgc, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select data from 2016-01-31 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.437484085351\n",
      "Select data from 2016-01-23 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.404582771329\n",
      "Select data from 2016-01-29 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.452958169253\n",
      "Select data from 2016-01-27 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.415666324248\n",
      "Select data from 2016-01-25 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.448645492101\n",
      "0.430986695795\n"
     ]
    }
   ],
   "source": [
    "score_on_test_data(Regr, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select important columns\n",
    "# columns = X_train.columns[[i for i, important in enumerate(rfc.feature_importances_ > 0.001) if important]].values\n",
    "columns = ['gap', 'last', '4', '13', '16', '23', '24', 'LV1', 'LV2', 'LV3', 'time']\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(newX[columns], newY['GAP'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={1: 38, 2: 37, 3: 36, 4: 35, 5: 34, 6: 33, 7: 32, 8: 31, 9: 30, 10: 29, 11: 28, 12: 27, 13: 26, 14: 25, 15: 24, 16: 23, 17: 22, 18: 21, 19: 20, 20: 19, 21: 18, 22: 17, 23: 16, 24: 15, 25: 14, 26: 13, 27: 12, 28: 11, 29: 10, 30: 9, 31: 8, 32: 7, 33: 6, 34: 5, 35: 4, 36: 3, 37: 2},\n",
       "            criterion='entropy', max_depth=9, max_features=0.85,\n",
       "            max_leaf_nodes=None, min_samples_leaf=10, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scoring_function = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "parameters = {'min_samples_split': np.arange(60, 140, 20)}\n",
    "\n",
    "# params = {'criterion': 'entropy', 'n_estimators':40, 'min_samples_leaf':10, 'min_samples_split':60,\n",
    "#           'max_depth':9, 'max_features':0.85, 'min_samples_split': 20, 'random_state':1,\n",
    "#           'verbose':0, 'class_weight': dict(zip(range(1,20), range(20,1,-1)))}\n",
    "\n",
    "# rfc = GridSearchCV(RandomForestClassifier(**params), parameters, scoring=scoring_function)\n",
    "rfc = RandomForestClassifier(**params)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.409512573031\n"
     ]
    }
   ],
   "source": [
    "print ((y_test - rfc.predict(X_test)).abs() / y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -21.25011, std: 0.05212, params: {'min_samples_split': 60},\n",
       " mean: -21.70977, std: 0.30694, params: {'min_samples_split': 80},\n",
       " mean: -21.44994, std: 0.08163, params: {'min_samples_split': 100},\n",
       " mean: -21.46662, std: 0.25857, params: {'min_samples_split': 120}]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 60}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_columns = columns + ['last2']\n",
    "newY = Y.copy()\n",
    "newX = X.copy()\n",
    "newX = newX[newY['GAP']>0]\n",
    "newY = newY[newY['GAP']>0]\n",
    "newY[newY['GAP']>39]=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={1: 20, 2: 19, 3: 18, 4: 17, 5: 16, 6: 15, 7: 14, 8: 13, 9: 12, 10: 11, 11: 10, 12: 9, 13: 8, 14: 7, 15: 6, 16: 5, 17: 4, 18: 3, 19: 2},\n",
       "            criterion='entropy', max_depth=9, max_features=0.85,\n",
       "            max_leaf_nodes=None, min_samples_leaf=10, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params = {'criterion': 'entropy', 'n_estimators':40, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "#           'max_depth':9, 'max_features':0.85, 'min_samples_split': 20, 'random_state':1,\n",
    "#           'verbose':0, 'class_weight': dict(zip(range(1,20), range(20,1,-1)))}\n",
    "\n",
    "rfc = RandomForestClassifier(**params)\n",
    "rfc.fit(newX[columns], newY['GAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_last_points_ans(day, slot):\n",
    "    slot = np.array(slot)\n",
    "    x = test_order[day].reindex(index(D_range, slot)).fillna(0)\n",
    "    x['last'] = test_order[day].reindex(index(D_range, slot-1)).fillna(0)\n",
    "    x['last2'] = test_order[day].reindex(index(D_range, slot-2)).fillna(0)\n",
    "    x = pd.concat([x, test_traffic[day].reindex(index(D_range, slot))],axis=1)\n",
    "    # For missing traffic data on district 54, replaced by Mask\n",
    "    for t in slot:\n",
    "        x.loc[(54,t)]['LV1':'weekday'] = x.loc[(Mask,t)]['LV1':'weekday']\n",
    "    x = x.join(test_weather[day], on='time')\n",
    "    x = x.join(POI,on='district')\n",
    "    print \"Select data from {} on {}\".format(day, slot)\n",
    "    print \"\\t shape: {}\".format(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write5(x, day, slot, rfc, mode):\n",
    "    with open('ans5_v2.csv', mode) as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for D in D_range:\n",
    "            for S in slot:\n",
    "                gap = rfc.predict(x.loc[(D,S)].reshape(1, -1))[0]\n",
    "                writer.writerow([str(D),'{}-{}'.format(day,S+1), '{:.15f}'.format(gap)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select data from 2016-01-31 on [ 45  57  69  81  93 105 117 129 141]\n",
      "\t shape: (594, 39)\n",
      "Select data from 2016-01-23 on [ 45  57  69  81  93 105 117 129 141]\n",
      "\t shape: (594, 39)\n",
      "Select data from 2016-01-29 on [ 57  69  81  93 105 117 129 141]\n",
      "\t shape: (528, 39)\n",
      "Select data from 2016-01-27 on [ 45  57  69  81  93 105 117 129 141]\n",
      "\t shape: (594, 39)\n",
      "Select data from 2016-01-25 on [ 57  69  81  93 105 117 129 141]\n",
      "\t shape: (528, 39)\n"
     ]
    }
   ],
   "source": [
    "for day in S_range.keys():\n",
    "    x = select_last_points_ans(day, S_range[day])\n",
    "    write5(x[columns], day, S_range[day], rfc, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method4 by GradientBoosting on classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "params = {'loss': 'deviance', 'learning_rate': 0.1, 'n_estimators': 50, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "          'max_depth': 3, 'max_features': 0.8, 'subsample': 1.0, 'min_samples_split': 20, 'random_state':1,\n",
    "          'verbose':1}\n",
    "grd = GradientBoostingClassifier(**params)\n",
    "grd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select data from 2016-01-31 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.468503175914\n",
      "Select data from 2016-01-23 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.476817064303\n",
      "Select data from 2016-01-29 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.514905471381\n",
      "Select data from 2016-01-27 on [ 44  56  68  80  92 104 116 128 140]\n",
      "\t shape: (594, 38)\n",
      "\t score: 0.471597751025\n",
      "Select data from 2016-01-25 on [ 56  68  80  92 104 116 128 140]\n",
      "\t shape: (528, 38)\n",
      "\t score: 0.488898065986\n",
      "0.483318377212\n"
     ]
    }
   ],
   "source": [
    "SLOT1 = range(44,152,12)\n",
    "SLOT2 = range(56,152,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "scores = []\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    y_pred = grd.predict(x[columns])\n",
    "    score = score_GBD(day, y_pred)\n",
    "    scores.append(score * x.shape[0])\n",
    "    print '\\t score: {}'.format(score)\n",
    "print np.array(scores).sum()/2838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'loss': 'deviance', 'learning_rate': 0.1, 'n_estimators': 50, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "          'max_depth': 3, 'max_features': 0.8, 'subsample': 1.0, 'min_samples_split': 20, 'random_state':1}\n",
    "clf = GradientBoostingClassifier(**params)\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i, important in enumerate(clf.feature_importances_ > 0.02):\n",
    "    if important:\n",
    "        columns.append(X.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'loss': 'deviance', 'learning_rate': 0.15, 'n_estimators': 50, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "          'max_depth': 3, 'max_features': 0.8, 'subsample': 1.0, 'min_samples_split': 20, 'random_state':1,\n",
    "          'verbose':1}\n",
    "clf = GradientBoostingClassifier(**params)\n",
    "clf.fit(X[columns],Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning rate: 0.1 -> 0.05 : 0.498091376721 -> 0.50989613439\n",
    "# learning rate: 0.1 -> 0.15 : 0.498091376721 -> 0.50989613439\n",
    "get_score(clf, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing ANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write4(x, day, slot, clf, mode):\n",
    "    with open('ans4_v1.csv', mode) as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for D in D_range:\n",
    "            for S in slot:\n",
    "                if D is 54:\n",
    "                    df = test_order[day].reindex(index(D_range,T_range)).fillna(0)['gap']\n",
    "                    gap = 0.5 * df.loc[(D, S)] + 0.3 * df.loc[(D, S-1)] + 0.2 * df.loc[(D, S-2)]\n",
    "                else:\n",
    "                    gap = clf.predict(x.loc[(D,S)].reshape(1, -1))[0]\n",
    "                gap = 0 if gap < 0 else gap\n",
    "                writer.writerow([str(D),'{}-{}'.format(day,S+1), '{:.15f}'.format(gap)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SLOT1 = range(45,153,12)\n",
    "SLOT2 = range(57,153,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "scores = []\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    write4(x[columns], day, S_range[day], clf, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformations with ensembles of trees (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score5(day, pred):\n",
    "    Ds = range(1,67)\n",
    "    Ds.reomve(54)\n",
    "    ans = test_order[day].reindex(index(D_range, S_range[day])).fillna(0)\n",
    "    ans = ans['gap'].values\n",
    "    pred = pred[ans>0]\n",
    "    ans = ans[ans>0]\n",
    "    gap = (ans - pred) / ans\n",
    "    # For district 54\n",
    "    another = test_order[day].reindex(index(54, S_range[day])).fillna(0)\n",
    "    \n",
    "    return np.fabs(gap).sum()/ans.shape[0]\n",
    "\n",
    "def get_score(clf, columns):\n",
    "    SLOT1 = range(44,152,12)\n",
    "    SLOT2 = range(56,152,12)\n",
    "    RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "    scores = []\n",
    "    for day in RANGE.keys():\n",
    "        x = select_last_points(day, RANGE[day])\n",
    "        score = score5(day, clf.predict(x[columns].dropna()))\n",
    "        scores.append(score * x.shape[0])\n",
    "        print '\\t score: {}'.format(score)\n",
    "    print np.array(scores).sum()/2838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = Y_gap[Y_gap>0]\n",
    "X = X[Y_gap>0]\n",
    "Y[Y>12]=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# columns = ['gap', 'last', 'LV1', 'LV2', 'LV3', 'LV4', 'district', 'time', 'pm2.5']\n",
    "new_columns = columns + ['last2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(X[columns], Y, test_size=0.7)\n",
    "\n",
    "params = {'loss': 'deviance', 'learning_rate': 0.15, 'n_estimators': 50, 'min_samples_leaf':10, 'min_samples_split':100,\n",
    "          'max_depth': 3, 'max_features': 0.8, 'subsample': 1.0, 'min_samples_split': 20, 'random_state':1,\n",
    "          'verbose':1}\n",
    "grd = GradientBoostingClassifier(**params)\n",
    "grd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = select_last_points(day, RANGE['2016-01-23'])\n",
    "print x[columns].loc[(54,RANGE['2016-01-23']),:]\n",
    "#print get_score(grd, columns)\n",
    "# print ((y_train_lr - grd.predict(X_train_lr)).abs() / y_train_lr).sum() / (66*y_train_lr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "enc = OneHotEncoder()\n",
    "lm_params = {'class_weight':{1:16,2:14,3:12,4:10,5:8}, 'solver':'newton-cg', 'random_state':1,\n",
    "             'penalty':'l2'}\n",
    "lm = LogisticRegression(**lm_params)\n",
    "enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "lm.fit(enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n",
    "# lm.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SLOT1 = range(44,152,12)\n",
    "SLOT2 = range(56,152,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "scores = []\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    y_pred_grd_lm = lm.predict_proba(enc.transform(grd.apply(x[columns])[:, :, 0]))[:, 1]\n",
    "    score = score3(day, pd.Series(y_pred_grd_lm, index=x.index))\n",
    "    scores.append(score * x.shape[0])\n",
    "    print '\\t score: {}'.format(score)\n",
    "print np.array(scores).sum()/2838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SLOT1 = range(44,152,12)\n",
    "SLOT2 = range(56,152,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "ans = pd.DataFrame()\n",
    "scores = pd.DataFrame()\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    y_pred_grd_lm = lm.predict_proba(enc.transform(grd.apply(x[columns])[:, :, 0]))[:, 1]\n",
    "    a, s = score4(day, pd.Series(y_pred_grd_lm, index=x.index))\n",
    "    ans = pd.concat([ans, a], axis=1)\n",
    "    scores = pd.concat([scores, s], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst = scores[(scores>=0.8).any(1)]\n",
    "print ans.loc[worst.index]['2016-01-23'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-25'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-27'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-29'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-31'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore which gap is the most common errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score4(day, pred):\n",
    "    a = test_order[day].reindex(index(D_range, S_range[day])).fillna(0)\n",
    "    ans = a['gap'].values\n",
    "    pred = pred[ans>0]\n",
    "    ans = ans[ans>0]\n",
    "    gap = (ans - pred) / ans\n",
    "    temp = pd.DataFrame(ans, index=pred.index,columns=[day])\n",
    "    return temp, pd.DataFrame(np.fabs(gap),columns=[day])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SLOT1 = range(44,152,12)\n",
    "SLOT2 = range(56,152,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "ans = pd.DataFrame()\n",
    "scores = pd.DataFrame()\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    a, s = score4(day, pd.Series(clf.predict(x[columns]), index=x.index))\n",
    "    ans = pd.concat([ans, a], axis=1)\n",
    "    scores = pd.concat([scores, s], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worst = scores[(scores>=0.8).any(1)]\n",
    "print ans.loc[worst.index]['2016-01-23'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-25'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-27'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-29'].value_counts()[:10]\n",
    "print ans.loc[worst.index]['2016-01-31'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method3 by GradientBoosting on regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "slot1 = range(45,153,12) # Last time slot of test slot for day 23, 27, 31\n",
    "slot2 = range(57,153,12) # Last time slot of test slot for day 25, 29\n",
    "S_range = {'2016-01-23':slot1, '2016-01-25':slot2, '2016-01-27':slot1, '2016-01-29':slot2, '2016-01-31':slot1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write3(x, day, slot, regr, mode):\n",
    "    with open('ans3_v1.csv', mode) as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for D in D_range:\n",
    "            for S in slot:\n",
    "                key = (D,S)\n",
    "                if key in x.index:\n",
    "                    gap = x['gap'].loc[key] + regr.predict(x.loc[key].reshape(1, -1))[0]\n",
    "                else:\n",
    "                    gap = 1\n",
    "                gap = 0 if gap < 0 else gap\n",
    "                writer.writerow([str(D),'{}-{}'.format(day,S+1), '{:.15f}'.format(gap)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ans3_v1.csv\n",
    "# params = {'loss': 'huber', 'alpha': 0.9, 'n_estimators':250, 'max_features':0.5, 'random_state':1,\n",
    "#           'warm_start':False,'max_depth': 3, 'learning_rate': 0.1, 'subsample': 1.0}\n",
    "#-----brand new turing\n",
    "params = {'loss': 'huber', 'alpha': 0.9, 'n_estimators':20, 'max_features':1.0, 'random_state':1,\n",
    "          'warm_start':False,'max_depth': 10, 'learning_rate': 0.25, 'subsample': 0.85,\n",
    "          'min_samples_leaf': 25, 'min_samples_split':100}\n",
    "\n",
    "# Paramters need to be searched\n",
    "parameters = {'n_estimators': np.arange(100,600,150)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Searching best parameters\n",
    "# scoring_function = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "# regr = grid_search.GridSearchCV(GradientBoostingRegressor(**params), \n",
    "#                                 param_grid=parameters, scoring=scoring_function, cv=3)\n",
    "# regr.fit(X, Y)\n",
    "# Regr = regr.best_estimator_\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "params = {'loss': 'huber', 'alpha': 0.9, 'n_estimators':250, 'max_features':0.5, 'random_state':1,\n",
    "          'warm_start':False,'max_depth': 3, 'learning_rate': 0.1, 'subsample': 1.0}\n",
    "Regr = GradientBoostingRegressor(**params)\n",
    "Regr.fit(X, Y)\n",
    "print Regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_features: sqrt -> 0.5 : 0.538287 -> 0.528706599387\n",
    "# max_features: 0.5 -> 0.8 : 0.528706599387 -> 0.530272920336\n",
    "#-----brand new turing\n",
    "# alpha: 0.9 -> 0.6 : 0.537699383041 -> 0.576867122284\n",
    "# learning_rate: 0.2 : 0.536744595218\n",
    "SLOT1 = range(44,152,12)\n",
    "SLOT2 = range(56,152,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "scores = []\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    score = score3(day, x['gap']+Regr.predict(x))\n",
    "    scores.append(score * x.shape[0])\n",
    "    print '\\t score: {}'.format(score)\n",
    "print np.array(scores).sum()/2838"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing ANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SLOT1 = range(45,153,12)\n",
    "SLOT2 = range(57,153,12)\n",
    "RANGE = {'2016-01-23': SLOT1, '2016-01-25': SLOT2, '2016-01-27': SLOT1, '2016-01-29': SLOT2, '2016-01-31': SLOT1}\n",
    "scores = []\n",
    "for day in RANGE.keys():\n",
    "    x = select_last_points(day, RANGE[day])\n",
    "    write3(x, day, S_range[day], Regr, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Method 2 by using interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slot1 = range(45,153,12)\n",
    "slot2 = range(57,153,12)\n",
    "S_range = {'2016-01-22':slot1, '2016-01-24':slot2, '2016-01-26':slot1, '2016-01-28':slot2, '2016-01-30':slot1}\n",
    "D_range = range(1,67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Day_range = {'2016-01-22':pd.date_range('1/2/2016', periods=3, freq='7D'),\n",
    "             '2016-01-24':pd.date_range('1/3/2016', periods=3, freq='7D'),\n",
    "             '2016-01-26':pd.date_range('1/5/2016', periods=3, freq='7D'),\n",
    "             '2016-01-28':pd.date_range('1/7/2016', periods=3, freq='7D'),\n",
    "             '2016-01-30':pd.date_range('1/9/2016', periods=2, freq='7D')}\n",
    "def naive_score2(day):\n",
    "    ans = test_order[day].select(lambda x: x[1] in S_range[day])\n",
    "    \n",
    "    \n",
    "    deltas = []\n",
    "    for d in Day_range[day]:\n",
    "        data = train_order[str(d.date())].reindex([x for x in itertools.product(D_range,range(1,145))]).fillna(0)\n",
    "        data = data.diff().shift(-1)\n",
    "        data = data.loc[ans.index]\n",
    "        deltas.append(data)\n",
    "\n",
    "    delta = deltas[0]\n",
    "    for i in range(1,len(deltas)):\n",
    "        delta.add(deltas[i], fill_value=0)\n",
    "    pred = test_order[day].shift().loc[ans.index].fillna(0)+(delta/len(deltas))\n",
    "    gap = (ans - pred) / ans\n",
    "\n",
    "    return gap.abs().sum()/(66*len(S_range[day]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print naive_score2('2016-01-22')\n",
    "print naive_score2('2016-01-24')\n",
    "print naive_score2('2016-01-26')\n",
    "print naive_score2('2016-01-28')\n",
    "print naive_score2('2016-01-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slope(day):\n",
    "    base_points = test_order[day].select(lambda x: x[1] in S_range[day])\n",
    "    deltas = []\n",
    "    for d in Day_range[day]:\n",
    "        data = train_order[str(d.date())].reindex([x for x in itertools.product(D_range,range(1,145))]).fillna(0)\n",
    "        data = data.diff().shift(-1)\n",
    "        deltas.append(data)\n",
    "    delta = deltas[0]\n",
    "    for i in range(1,len(deltas)):\n",
    "        delta = delta + deltas[i]\n",
    "    return base_points, delta/len(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write2(day, base_points, slot, delta, mode):\n",
    "    with open('ans.csv', mode) as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for D in range(1,67):\n",
    "            for S in slot:\n",
    "                key = (D,S-1)\n",
    "                if key in base_points.index:\n",
    "                    gap = base_points['gap'].loc[key] + delta['gap'].loc[key]\n",
    "                    gap = base_points['gap'].loc[key] if gap < 0 else gap\n",
    "                else:\n",
    "                    gap = 0.0\n",
    "                writer.writerow([D,'{}-{}'.format(day,S), '{:.3f}'.format(gap)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = '2016-01-22'\n",
    "base_points, delta = slope(day)\n",
    "write2(day, base_points, test_slot1, delta, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = '2016-01-24'\n",
    "base_points, delta = slope(day)\n",
    "write2(day, base_points, test_slot2, delta, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = '2016-01-26'\n",
    "base_points, delta = slope(day)\n",
    "write2(day, base_points, test_slot1, delta, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = '2016-01-28'\n",
    "base_points, delta = slope(day)\n",
    "write2(day, base_points, test_slot2, delta, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day = '2016-01-30'\n",
    "base_points, delta = slope(day)\n",
    "write2(day, base_points, test_slot1, delta, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the score of naive method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Day_range = {'2016-01-22':pd.date_range('1/1/2016', periods=3, freq='7D'),\n",
    "             '2016-01-24':pd.date_range('1/3/2016', periods=3, freq='7D'),\n",
    "             '2016-01-26':pd.date_range('1/5/2016', periods=3, freq='7D'),\n",
    "             '2016-01-28':pd.date_range('1/7/2016', periods=3, freq='7D'),\n",
    "             '2016-01-30':pd.date_range('1/9/2016', periods=2, freq='7D')}\n",
    "def naive_score1(day):\n",
    "    ans = test_order[day].select(lambda x: x[1] in S_range[day])\n",
    "    \n",
    "    prediction = []\n",
    "    for d in Day_range[day]:\n",
    "        data = train_order[str(d.date())]\n",
    "        data = data.loc[ans.index].fillna(0)\n",
    "        prediction.append(data)\n",
    "\n",
    "    pred = prediction[0]\n",
    "    for i in range(1,len(prediction)):\n",
    "        pred.add(prediction[i], fill_value=0)\n",
    "    pred = pred/len(prediction)\n",
    "    gap = (ans - pred) / ans\n",
    "\n",
    "    return gap.abs().sum()/(66*len(S_range[day]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print naive_score1('2016-01-22')\n",
    "print naive_score1('2016-01-24')\n",
    "print naive_score1('2016-01-26')\n",
    "print naive_score1('2016-01-28')\n",
    "print naive_score1('2016-01-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Method 1 by using mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_slot1 = range(46,154,12)\n",
    "test_slot2 = range(58,154,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_day22 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in pd.date_range('1/1/2016', periods=3, freq='7D'):\n",
    "    # Read data\n",
    "    order = pd.read_table(path['order'].format(day.date()), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district', 'time'])\n",
    "    # Select NA for calculating the value of gap\n",
    "    order = order[order['driver'].isnull()] \n",
    "    # Translating hash to id\n",
    "    order['district'] = order['district'].apply(lambda x: district[x])\n",
    "    # Translating timestamp to time_id\n",
    "    order['time'] = pd.to_datetime(order['time'])\n",
    "    order['time'] = (order['time'] - day) / M / 10 + 1\n",
    "    order['time'] = order['time'].astype(int)\n",
    "    # Grouping by district and time\n",
    "    test_day22.append(order.groupby(['district', 'time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ans.csv', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for D in range(1,67):\n",
    "        for S in test_slot1:\n",
    "            key = (D,S)\n",
    "            gap = []\n",
    "            for data in test_day22:\n",
    "                if key in data.groups:\n",
    "                    gap.append(data.get_group(key).shape[0])\n",
    "                else:\n",
    "                    gap.append(0)\n",
    "            writer.writerow([D,'2016-01-22-{}'.format(S), '{:.3f}'.format(np.mean(gap))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_day24 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in pd.date_range('1/3/2016', periods=3, freq='7D'):\n",
    "    # Read data\n",
    "    order = pd.read_table(path['order'].format(day.date()), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district', 'time'])\n",
    "    # Select NA for calculating the value of gap\n",
    "    order = order[order['driver'].isnull()] \n",
    "    # Translating hash to id\n",
    "    order['district'] = order['district'].apply(lambda x: district[x])\n",
    "    # Translating timestamp to time_id\n",
    "    order['time'] = pd.to_datetime(order['time'])\n",
    "    order['time'] = (order['time'] - day) / M / 10 + 1\n",
    "    order['time'] = order['time'].astype(int)\n",
    "    # Grouping by district and time\n",
    "    test_day24.append(order.groupby(['district', 'time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ans.csv', 'a') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for D in range(1,67):\n",
    "        for S in test_slot2:\n",
    "            key = (D,S)\n",
    "            gap = []\n",
    "            for data in test_day24:\n",
    "                if key in data.groups:\n",
    "                    gap.append(data.get_group(key).shape[0])\n",
    "                else:\n",
    "                    gap.append(0)\n",
    "            writer.writerow([D,'2016-01-24-{}'.format(S), '{:.3f}'.format(np.mean(gap))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_day26 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in pd.date_range('1/5/2016', periods=3, freq='7D'):\n",
    "    # Read data\n",
    "    order = pd.read_table(path['order'].format(day.date()), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district', 'time'])\n",
    "    # Select NA for calculating the value of gap\n",
    "    order = order[order['driver'].isnull()] \n",
    "    # Translating hash to id\n",
    "    order['district'] = order['district'].apply(lambda x: district[x])\n",
    "    # Translating timestamp to time_id\n",
    "    order['time'] = pd.to_datetime(order['time'])\n",
    "    order['time'] = (order['time'] - day) / M / 10 + 1\n",
    "    order['time'] = order['time'].astype(int)\n",
    "    # Grouping by district and time\n",
    "    test_day26.append(order.groupby(['district', 'time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ans.csv', 'a') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for D in range(1,67):\n",
    "        for S in test_slot1:\n",
    "            key = (D,S)\n",
    "            gap = []\n",
    "            for data in test_day26:\n",
    "                if key in data.groups:\n",
    "                    gap.append(data.get_group(key).shape[0])\n",
    "                else:\n",
    "                    gap.append(0)\n",
    "            writer.writerow([D,'2016-01-26-{}'.format(S), '{:.3f}'.format(np.mean(gap))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_day28 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in pd.date_range('1/7/2016', periods=3, freq='7D'):\n",
    "    # Read data\n",
    "    order = pd.read_table(path['order'].format(day.date()), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district', 'time'])\n",
    "    # Select NA for calculating the value of gap\n",
    "    order = order[order['driver'].isnull()] \n",
    "    # Translating hash to id\n",
    "    order['district'] = order['district'].apply(lambda x: district[x])\n",
    "    # Translating timestamp to time_id\n",
    "    order['time'] = pd.to_datetime(order['time'])\n",
    "    order['time'] = (order['time'] - day) / M / 10 + 1\n",
    "    order['time'] = order['time'].astype(int)\n",
    "    # Grouping by district and time\n",
    "    test_day28.append(order.groupby(['district', 'time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ans.csv', 'a') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for D in range(1,67):\n",
    "        for S in test_slot2:\n",
    "            key = (D,S)\n",
    "            gap = []\n",
    "            for data in test_day28:\n",
    "                if key in data.groups:\n",
    "                    gap.append(data.get_group(key).shape[0])\n",
    "                else:\n",
    "                    gap.append(0)\n",
    "            writer.writerow([D,'2016-01-28-{}'.format(S), '{:.3f}'.format(np.mean(gap))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Day30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_day30 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for day in pd.date_range('1/9/2016', periods=2, freq='7D'):\n",
    "    # Read data\n",
    "    order = pd.read_table(path['order'].format(day.date()), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district', 'time'])\n",
    "    # Select NA for calculating the value of gap\n",
    "    order = order[order['driver'].isnull()] \n",
    "    # Translating hash to id\n",
    "    order['district'] = order['district'].apply(lambda x: district[x])\n",
    "    # Translating timestamp to time_id\n",
    "    order['time'] = pd.to_datetime(order['time'])\n",
    "    order['time'] = (order['time'] - day) / M / 10 + 1\n",
    "    order['time'] = order['time'].astype(int)\n",
    "    # Grouping by district and time\n",
    "    test_day30.append(order.groupby(['district', 'time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('ans.csv', 'a') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for D in range(1,67):\n",
    "        for S in test_slot1:\n",
    "            key = (D,S)\n",
    "            gap = []\n",
    "            for data in test_day30:\n",
    "                if key in data.groups:\n",
    "                    gap.append(data.get_group(key).shape[0])\n",
    "                else:\n",
    "                    gap.append(0)\n",
    "            writer.writerow([D,'2016-01-30-{}'.format(S), '{:.3f}'.format(np.mean(gap))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day = '2016-01-01'\n",
    "Day = pd.Timestamp(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order = pd.read_table(path['order'].format(day), header=None, usecols=[1,3,6],\n",
    "                      names=['driver', 'district_id', 'time'])\n",
    "order = order[order['driver'].isnull()] # Select NA for calculating the value of gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating district hash to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order['district_id'] = order['district_id'].apply(lambda x: district[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating timestamp to slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order['time'] = pd.to_datetime(order['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order['time_slot'] = (order['time'] - Day) / M / 10 + 1\n",
    "order['time_slot'] = order['time_slot'].astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
